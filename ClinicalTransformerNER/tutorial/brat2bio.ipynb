{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example for converting brat to BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use chia dataset as example\n",
    "# https://www.nature.com/articles/s41597-020-00620-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath = f\"chia/chia_with_scope\"\n",
    "outputpath = f\"chia/chia_bio\"\n",
    "trainpath = f\"chia/trains\"\n",
    "testpath = f\"chia/tests\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfiles = set()\n",
    "for f in os.listdir(inputpath):\n",
    "    if f.endswith('.ann'):\n",
    "        inputfiles.add(f.split('.')[0].split('_')[0])\n",
    "len(inputfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of entity types to retain\n",
    "select_types = ['Condition', 'Value', 'Drug', 'Procedure', 'Measurement', 'Temporal', \\\n",
    "    'Observation', 'Person', 'Mood', 'Device', 'Pregnancy_considerations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Brat format into BIO format\n",
    "# function for getting entity annotations from the annotation file\n",
    "def get_annotation_entities(ann_file, select_types=None):\n",
    "    entities = []\n",
    "    with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith('T'):\n",
    "                term = line.strip().split('\\t')[1].split()\n",
    "                if (select_types != None) and (term[0] not in select_types): continue\n",
    "                if int(term[-1]) <= int(term[1]): continue\n",
    "                entities.append((int(term[1]), int(term[-1]), term[0]))\n",
    "    return sorted(entities, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "# function for handling overlap by keeping the entity with largest text span\n",
    "def remove_overlap_entities(sorted_entities):\n",
    "    keep_entities = []\n",
    "    for idx, entity in enumerate(sorted_entities):\n",
    "        if idx == 0:\n",
    "            keep_entities.append(entity)\n",
    "            last_keep = entity\n",
    "            continue\n",
    "        if entity[0] < last_keep[1]:\n",
    "            if entity[1]-entity[0] > last_keep[1]-last_keep[0]:\n",
    "                last_keep = entity\n",
    "                keep_entities[-1] = last_keep\n",
    "        elif entity[0] == last_keep[1]:\n",
    "            last_keep = (last_keep[0], entity[1], last_keep[-1])\n",
    "            keep_entities[-1] = last_keep\n",
    "        else:\n",
    "            last_keep = entity\n",
    "            keep_entities.append(entity)\n",
    "    return keep_entities\n",
    "\n",
    "# inverse index of entity annotations\n",
    "def entity_dictionary(keep_entities, txt_file):\n",
    "    f_ann = {}\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()\n",
    "        if file in ['NCT02348918_exc', 'NCT02348918_inc', 'NCT01735955_exc']:\n",
    "            text = ' '.join([i.strip() for i in text])\n",
    "        else:\n",
    "            text = '  '.join([i.strip() for i in text])\n",
    "    for entity in keep_entities:\n",
    "        entity_text = text[entity[0]:entity[1]]\n",
    "        doc = nlp(entity_text)\n",
    "        token_starts = [(i, doc[i:].start_char) for i in range(len(doc))]\n",
    "        term_type = entity[-1]\n",
    "        term_offset = entity[0]\n",
    "        for i, token in enumerate(doc):\n",
    "            ann_offset = token_starts[i][1]+term_offset\n",
    "            if ann_offset not in f_ann:\n",
    "                f_ann[ann_offset] = [i, token.text, term_type]\n",
    "    return f_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brat -> BIO format conversion\n",
    "for infile in inputfiles:\n",
    "    for t in [\"exc\", \"inc\"]:\n",
    "        file = f\"{infile}_{t}\"\n",
    "        ann_file = f\"{inputpath}/{file}.ann\"\n",
    "        txt_file = f\"{inputpath}/{file}.txt\"\n",
    "        out_file = f\"{outputpath}/{file}.bio.txt\"\n",
    "        sorted_entities = get_annotation_entities(ann_file, select_types)\n",
    "        keep_entities = remove_overlap_entities(sorted_entities)\n",
    "        f_ann = entity_dictionary(keep_entities, txt_file)\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "            with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                sent_offset = 0\n",
    "                for line in f:\n",
    "                    if '⁄' in line:\n",
    "                        line = line.replace('⁄', '/') # replace non unicode characters\n",
    "                    doc = nlp(line.strip())\n",
    "                    token_starts = [(i, doc[i:].start_char) for i in range(len(doc))]\n",
    "                    for token in doc:\n",
    "                        token_sent_offset = token_starts[token.i][1]\n",
    "                        token_doc_offset = token_starts[token.i][1]+sent_offset\n",
    "                        if token_doc_offset in f_ann:\n",
    "                            if f_ann[token_doc_offset][0] == 0:\n",
    "                                label = f\"B-{f_ann[token_doc_offset][2]}\"\n",
    "                            else:\n",
    "                                label = f\"I-{f_ann[token_doc_offset][2]}\"\n",
    "                        else:\n",
    "                            label = f\"O\"\n",
    "                        # print(token.text, token_sent_offset, token_sent_offset+len(token.text), token_doc_offset, token_doc_offset+len(token.text), label)\n",
    "                        f_out.write(f\"{token.text} {token_sent_offset} {token_sent_offset+len(token.text)} {token_doc_offset} {token_doc_offset+len(token.text)} {label}\\n\")\n",
    "                    # print('\\n')\n",
    "                    f_out.write('\\n')\n",
    "                    if file in ['NCT02348918_exc', 'NCT02348918_inc', 'NCT01735955_exc']: # 3 trials with inconsistent offsets\n",
    "                        sent_offset += (len(line.strip())+1)\n",
    "                    else:\n",
    "                        sent_offset += (len(line.strip())+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset separation: 800 trials (80%) for training, 100 trials (10%) for validation and 100 trials (10%) for testing\n",
    "train_ids, dev_ids = train_test_split(list(inputfiles), train_size=0.8, random_state=13, shuffle=True)\n",
    "dev_ids, test_ids = train_test_split(dev_ids, train_size=0.5, random_state=13, shuffle=True)\n",
    "print(len(train_ids), len(dev_ids), len(test_ids))\n",
    "chia_datasets = {\"train\":train_ids, \"dev\":dev_ids, \"test\":test_ids}\n",
    "json.dump(chia_datasets, open(\"chia/chia_datasets.json\", \"w\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge BIO format train, validation and test datasets\n",
    "# merge the train dataset\n",
    "with open(\"chia/train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for fid in chia_datasets[\"train\"]:\n",
    "        copyfile(f\"{outputpath}/{fid}_exc.bio.txt\", f\"{trainpath}/{fid}_exc.bio.txt\")\n",
    "        copyfile(f\"{outputpath}/{fid}_inc.bio.txt\", f\"{trainpath}/{fid}_inc.bio.txt\")\n",
    "        with open(f\"{outputpath}/{fid}_exc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "        with open(f\"{outputpath}/{fid}_inc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "# merge the validation dataset\n",
    "with open(\"chia/dev.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for fid in chia_datasets[\"dev\"]:\n",
    "        copyfile(f\"{outputpath}/{fid}_exc.bio.txt\", f\"{trainpath}/{fid}_exc.bio.txt\")\n",
    "        copyfile(f\"{outputpath}/{fid}_inc.bio.txt\", f\"{trainpath}/{fid}_inc.bio.txt\")\n",
    "        with open(f\"{outputpath}/{fid}_exc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "        with open(f\"{outputpath}/{fid}_inc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "# merge the test dataset\n",
    "with open(\"chia/test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for fid in chia_datasets[\"test\"]:\n",
    "        copyfile(f\"{outputpath}/{fid}_exc.bio.txt\", f\"{testpath}/{fid}_exc.bio.txt\")\n",
    "        copyfile(f\"{outputpath}/{fid}_inc.bio.txt\", f\"{testpath}/{fid}_inc.bio.txt\")\n",
    "        with open(f\"{outputpath}/{fid}_exc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "        with open(f\"{outputpath}/{fid}_inc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
