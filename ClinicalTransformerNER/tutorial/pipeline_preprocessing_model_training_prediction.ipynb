{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A full example of how to use the package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T21:05:40.901597Z",
     "start_time": "2021-08-18T21:05:40.899386Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will use DDI data set at https://github.com/isegura/DDICorpus\n",
    "training_root = \"./ddi/DDICorpusBrat/Train/MedLine\"\n",
    "testing_root = \"./ddi/DDICorpusBrat/Test/MedLine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing using NLPpreprocessing package\n",
    "\n",
    "> if you do not want to use this package, you can see the tutorial brat2bio.ipynb or https://github.com/nlplab/brat/blob/master/tools/anntoconll.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T19:46:47.761329Z",
     "start_time": "2021-08-18T19:46:46.417757Z"
    }
   },
   "outputs": [],
   "source": [
    "# download preprocessing package NLP\n",
    "! git clone https://github.com/uf-hobi-informatics-lab/NLPreprocessing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T19:58:17.989548Z",
     "start_time": "2021-08-18T19:58:17.974345Z"
    }
   },
   "outputs": [],
   "source": [
    "# link pacakge to python path\n",
    "# import necessary functions\n",
    "import sys\n",
    "sys.path.append(\"./NLPreprocessing\")\n",
    "sys.path.append(\"./NLPreprocessing/text_process\")\n",
    "sys.path.append(\"./NLPpreprocessing/text_process/sentence_tokenization.py\")\n",
    "\n",
    "import logging\n",
    "from annotation2BIO import generate_BIO, pre_processing, read_annotation_brat, BIOdata_to_file, logger\n",
    "from sentence_tokenization import logger as logger1\n",
    "\n",
    "# change log level to error to avoid too much log information in jupyter notebook\n",
    "logger1.setLevel(logging.ERROR)\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:53:34.396303Z",
     "start_time": "2021-08-18T20:53:34.390390Z"
    }
   },
   "outputs": [],
   "source": [
    "file_ids = set()\n",
    "\n",
    "for fn in Path(training_root).glob(\"*.ann\"):\n",
    "    file_ids.add(fn.stem)\n",
    "    \n",
    "len(file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:54:05.158495Z",
     "start_time": "2021-08-18T20:53:56.605891Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate BIO from brat annotation\n",
    "train_root = Path(training_root)\n",
    "# train_bio = \"./2018n2c2/bio/trains\"\n",
    "train_bio = \"./ddi/ddi_bio/\"\n",
    "output_root = Path(train_bio)\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fid in file_ids:\n",
    "    txt_fn = train_root / (fid + \".txt\")\n",
    "    ann_fn = train_root / (fid + \".ann\")\n",
    "    bio_fn = output_root / (fid + \".bio.txt\")\n",
    "    \n",
    "    txt, sents = pre_processing(txt_fn)\n",
    "    e2idx, entities, rels = read_annotation_brat(ann_fn)\n",
    "    nsents, sent_bound = generate_BIO(sents, entities, file_id=fid, no_overlap=False)\n",
    "    \n",
    "    BIOdata_to_file(bio_fn, nsents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:55:30.793834Z",
     "start_time": "2021-08-18T20:55:30.103629Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we have to split the train and dev sets\n",
    "# for transformer NER, we need to name these two datasets as train.txt and dev.txt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_ids = list(file_ids)\n",
    "train_ids, dev_ids = train_test_split(file_ids, train_size=0.9, random_state=13, shuffle=True)\n",
    "len(train_ids), len(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T20:56:16.667473Z",
     "start_time": "2021-08-18T20:56:16.626450Z"
    }
   },
   "outputs": [],
   "source": [
    "import fileinput\n",
    "\n",
    "merged = output_root / \"merge\" # this will the final data dir we use for training\n",
    "merged.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# train\n",
    "with open(merged / \"train.txt\", \"w\") as f:\n",
    "    for fid in train_ids:\n",
    "        f.writelines(fileinput.input(output_root / (fid + \".bio.txt\")))\n",
    "    fileinput.close()\n",
    "        \n",
    "# dev\n",
    "with open(merged /\"dev.txt\", \"w\") as f:\n",
    "    for fid in dev_ids:\n",
    "        f.writelines(fileinput.input(output_root / (fid + \".bio.txt\")))\n",
    "    fileinput.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T21:01:52.395035Z",
     "start_time": "2021-08-18T21:01:31.980908Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next we will train the NER model\n",
    "\n",
    "We will just use a BERT model pre-trained on general English corpora as an example\n",
    "\n",
    "In general we need GPU to train the model, running with CPU the training will be extremely slow. \n",
    "\n",
    "To use GPU, you just need to run 'export CUDA_VISIBLE_DEVICES=0' before run the training command\n",
    "\"\"\"\n",
    "\n",
    "# -1 indicates using CPU for training; \n",
    "# 0 indicate we use the GPU with ID as 0, etc.\n",
    "! export CUDA_VISIBLE_DEVICES=0 \n",
    "\n",
    "# this is just an example, please refer to the readme for how to set hyperparameters\n",
    "! python ../src/run_transformer_ner.py \\\n",
    "      --model_type bert \\\n",
    "      --pretrained_model bert-base-uncased \\\n",
    "      --data_dir ./ddi/ddi_bio/merge \\\n",
    "      --new_model_dir ./new_bert_ner_model \\\n",
    "      --overwrite_model_dir \\\n",
    "      --max_seq_length 128 \\\n",
    "      --data_has_offset_information \\\n",
    "      --save_model_core \\\n",
    "      --do_train \\\n",
    "      --model_selection_scoring strict-f_score-1 \\\n",
    "      --do_lower_case \\\n",
    "      --train_batch_size 8 \\\n",
    "      --train_steps 1000 \\\n",
    "      --learning_rate 1e-5 \\\n",
    "      --num_train_epochs 20 \\\n",
    "      --gradient_accumulation_steps 1 \\\n",
    "      --do_warmup \\\n",
    "      --seed 13 \\\n",
    "      --warmup_ratio 0.1 \\\n",
    "      --max_num_checkpoints 1 \\\n",
    "      --log_file ./log.txt \\\n",
    "      --progress_bar \\\n",
    "      --early_stop 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do prediction on each test set file and format prediction as brat output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T21:07:04.934854Z",
     "start_time": "2021-08-18T21:06:59.577336Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "running prediction\n",
    "\n",
    "In our transformer package, we have the format conversion between bio and brat; bioc implemented\n",
    "\n",
    "you still have to convert the txt files for prediction to BIO format,\n",
    "but here you do not need to assign a real annotation label, we just use O as dummy\n",
    "\"\"\"\n",
    "\n",
    "# generate bio\n",
    "test_root = Path(testing_root)\n",
    "test_bio = \"./ddi/ddi_bio/test\"\n",
    "output_root = Path(test_bio)\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fn in test_root.glob(\"*.txt\"):\n",
    "    txt_fn = fn\n",
    "    bio_fn = output_root / (fn.stem + \".bio.txt\")\n",
    "    \n",
    "    txt, sents = pre_processing(txt_fn)\n",
    "    annotations = [] # here we just use an empty list for annotation so that all words will be labeled as O\n",
    "    nsents, sent_bound = generate_BIO(sents, annotations, file_id=fid, no_overlap=False)\n",
    "    \n",
    "    BIOdata_to_file(bio_fn, nsents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction\n",
    "! export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# use format 1 for BRAT, 2 for BioC, 0 as default for BIO\n",
    "# see readme for more information on parameters\n",
    "! python ../ClinicalTransformerNER/src/run_transformer_batch_prediction.py \\\n",
    "      --model_type bert \\\n",
    "      --pretrained_model ./new_bert_ner_model \\\n",
    "      --raw_text_dir $testing_root \\ # this is the dir where original text file located\n",
    "      --preprocessed_text_dir $test_bio \\ # this is the dir where the BIO file located\n",
    "      --output_dir ./ddi/results \\\n",
    "      --max_seq_length 128 \\\n",
    "      --do_lower_case \\\n",
    "      --eval_batch_size 8 \\\n",
    "      --log_file ./log.txt\\\n",
    "      --do_format 1 \\\n",
    "      --do_copy \\\n",
    "      --data_has_offset_information\n",
    "\n",
    "# the bio prediction output will be generateed in ./ddi/results\n",
    "# the brat prediction output will be generateed in ./ddi/results_formatted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation using brat eval script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation\n",
    "# we have a brat_eval.py script used for evaluation of NER and relation extraction based on brat format\n",
    "\n",
    "! python ../src/eval_scripts/brat_eval.py --f1 ./ddi/DDICorpusBrat/Test/MedLine --f2 ./ddi/results_formatted_output/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
