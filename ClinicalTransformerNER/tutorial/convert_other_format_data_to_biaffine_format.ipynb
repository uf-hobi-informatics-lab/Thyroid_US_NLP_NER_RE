{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07455f5",
   "metadata": {},
   "source": [
    "## Demo on how to generate data from BRAT or BIO format for biaffine training\n",
    "\n",
    "- the default format of the data is below, we only read en_text, en_type, start, end indexes, you can add other information in entities after these elements for post-processing\n",
    "```\n",
    "[\n",
    "    {\"tokens\": [xx, xx, xx, ...], \n",
    "    \"entities\": [[en_text, en_type, (start_idx, end_idx)], [en_text, en_type, (start_idx, end_idx)], ...]\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "- note the start_idx, end_idx are token index in tokens not the absolute position in the original document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31cb6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:53:30.303989Z",
     "start_time": "2022-03-04T03:53:30.301074Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a5d8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:53:30.490704Z",
     "start_time": "2022-03-04T03:53:30.488446Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# get NLPpreprocessing from https://github.com/uf-hobi-informatics-lab/NLPreprocessing\n",
    "sys.path.append(\"./NLPpreprocessing/\")\n",
    "sys.path.append(\"./NLPpreprocessing/text_process/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407238d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:53:30.689237Z",
     "start_time": "2022-03-04T03:53:30.671044Z"
    }
   },
   "outputs": [],
   "source": [
    "from annotation2BIO import pre_processing, read_annotation_brat\n",
    "from annotation2BIO import logger as l1\n",
    "from sentence_tokenization import logger as l2\n",
    "l1.setLevel(logging.ERROR)\n",
    "l2.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d9b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:53:32.233117Z",
     "start_time": "2022-03-04T03:53:32.223337Z"
    }
   },
   "outputs": [],
   "source": [
    "def json_dump(data, fn):\n",
    "    with open(fn, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "\n",
    "def pkl_dump(pdata, ofn):\n",
    "    with open(ofn, \"wb\") as f:\n",
    "        pkl.dump(pdata, f)\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_sent_bound(sents):\n",
    "    sent_bound_range = dict()  # key: sent id; value: boundary range\n",
    "    for i, each in enumerate(sents):\n",
    "        try:\n",
    "            sent_start_index = each[0][1][0]\n",
    "            sent_end_index = each[-1][1][1]\n",
    "            sent_bound_range[i] = (sent_start_index, sent_end_index)\n",
    "        except Exception as ex:\n",
    "            if i != len(nsents) - 1:\n",
    "                raise RuntimeError(f'The {i}th sentence is an empty sentence')\n",
    "    return sent_bound_range\n",
    "\n",
    "\n",
    "def get_sent_idx(en, r2i, fid):\n",
    "    s, e = en[2]\n",
    "    for r in r2i:\n",
    "        ss, se = r\n",
    "        if ss <= s < e <= se:\n",
    "            return r2i[r]\n",
    "    \n",
    "    warnings.warn(f\"entity {en} in {fid} - cannot be mapped to one sentence. Will skip this entity.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_en_idx_in_sent(en, sent):\n",
    "    s, e = None, None\n",
    "    for idx, word in enumerate(sent):\n",
    "        if word[1][0] == en[2][0]:\n",
    "            s = idx\n",
    "        if word[1][1] == en[2][1]:\n",
    "            e = idx\n",
    "    if s == None:\n",
    "        for idx, word in enumerate(sent):\n",
    "            if word[1][0] < en[2][0] < word[1][1]:\n",
    "                s = idx\n",
    "                break\n",
    "    if e == None:\n",
    "        for idx, word in enumerate(sent):\n",
    "            if word[1][0] < en[2][1] < word[1][1]:\n",
    "                e = idx\n",
    "                break\n",
    "    assert s != None and e != None, f\"{en}\\n{sent}\"\n",
    "    return s, e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f37d13",
   "metadata": {},
   "source": [
    "## training data from brat to biaffine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0193158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:48:50.276066Z",
     "start_time": "2022-03-02T03:48:50.272384Z"
    }
   },
   "outputs": [],
   "source": [
    "# biaffine entity format (text, type, start, end) => \n",
    "# later we need to translate the start end to indexes in sentence to construct labels and mask\n",
    "# skip BIO to overcome overlap entities issue\n",
    "# final formatted data should be json lines {sent: \"xxxx\", entity: [(en1, ty, s, e, sindex, eindex), ...]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359b76c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:53:50.834915Z",
     "start_time": "2022-03-04T03:53:50.820378Z"
    }
   },
   "outputs": [],
   "source": [
    "# we use 2018 n2c2 data as brat example, you can replace with any brat formatted data here\n",
    "p = Path(\"./data/2018_n2c2_ade/track2-training_data/\")\n",
    "\n",
    "fids = [fn.stem for fn in p.glob(\"*.ann\")]\n",
    "fids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573595ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:53:52.625112Z",
     "start_time": "2022-03-04T03:53:52.620075Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, dev_ids = train_test_split(fids, train_size=0.9, random_state=13, shuffle=True)\n",
    "len(train_ids), len(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99389b3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:53:53.214586Z",
     "start_time": "2022-03-04T03:53:53.206888Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def brat2biaffine_data(file_ids, file_path, test=False):\n",
    "    biaffine_data = []\n",
    "    mappings = []\n",
    "\n",
    "    for fid in file_ids:\n",
    "        t_fn = file_path / f\"{fid}.txt\"\n",
    "        a_fn = file_path / f\"{fid}.ann\"\n",
    "        \n",
    "        if test:\n",
    "            ens, rels = [], []\n",
    "        else:\n",
    "            enid_map, ens, rels = read_annotation_brat(a_fn)\n",
    "        \n",
    "        sents_text, sents = pre_processing(t_fn, max_len=200)\n",
    "\n",
    "        set_bound = get_sent_bound(sents)\n",
    "        range2idx = {v:k for k, v in set_bound.items()}\n",
    "        \n",
    "        en_track = defaultdict(list)\n",
    "        for en in ens:\n",
    "            sent_idx = get_sent_idx(en, range2idx, fid)\n",
    "            if not sent_idx:\n",
    "                # skip the en that cannot be mapped\n",
    "                continue\n",
    "            # get start end index in sentence; will be updated\n",
    "            sent = sents[sent_idx]\n",
    "            s_idx, e_idx = get_en_idx_in_sent(en, sent)\n",
    "            # cat reduce to only type and index for training to reduce saved data size\n",
    "            # en[2], sent_idx, fid : these are not necessary\n",
    "            en_track[sent_idx].append([en[0], en[1], (s_idx, e_idx)])\n",
    "    \n",
    "        # we still need to keep sentences that have no entities\n",
    "        for i in range(len(sents)):\n",
    "            biaffine_data.append(\n",
    "                {\n",
    "                    \"tokens\": [e[0] for e in sents[i]], \n",
    "                    \"entities\": sorted(en_track[i], key=lambda x: x[2][0])\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # mappings.extend(sents)\n",
    "        # in batch prediction we do not need this since we do not need to track fid\n",
    "        # this is only need when we need to predict and track file id\n",
    "        if test:\n",
    "            nnsents = []\n",
    "            for sent in sents:\n",
    "                nnsent = []\n",
    "                for each in sent:\n",
    "                    nnsent.append([*each, fid])\n",
    "                nnsents.append(nnsent)\n",
    "        \n",
    "        mappings.extend(nnsents)\n",
    "\n",
    "    return biaffine_data, mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5725a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:54:29.923689Z",
     "start_time": "2022-03-04T03:53:58.122767Z"
    }
   },
   "outputs": [],
   "source": [
    "biaffine_train, _ = brat2biaffine_data(train_ids, p)\n",
    "biaffine_dev, _ = brat2biaffine_data(dev_ids, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d433e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:54:48.752179Z",
     "start_time": "2022-03-04T03:54:29.925664Z"
    }
   },
   "outputs": [],
   "source": [
    "p_test = Path(\"./data/2018_n2c2_ade/gold_standard_test/\")\n",
    "\n",
    "test_fids = [fn.stem for fn in p_test.glob(\"*.txt\")]\n",
    "biaffine_test, test_mappings = brat2biaffine_data(test_fids, p_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643cd6fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:54:48.757607Z",
     "start_time": "2022-03-04T03:54:48.753635Z"
    }
   },
   "outputs": [],
   "source": [
    "len(biaffine_train), len(biaffine_dev), len(biaffine_test), len(test_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d87248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:59:52.104106Z",
     "start_time": "2022-03-04T03:59:52.100408Z"
    }
   },
   "outputs": [],
   "source": [
    "test_mappings[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6449acd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-01T20:38:01.694Z"
    }
   },
   "outputs": [],
   "source": [
    "json_dump(biaffine_train, \"./data/n2c2/train.json\")\n",
    "json_dump(biaffine_dev, \"./data/n2c2/dev.json\")\n",
    "json_dump(biaffine_test, \"./data/n2c2/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244fad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:59:23.897674Z",
     "start_time": "2022-03-04T03:59:23.278408Z"
    }
   },
   "outputs": [],
   "source": [
    "# for brat format\n",
    "# when generate test, we also need to generate a token offset mapping file\n",
    "pkl_dump(test_mappings, \"./data/n2c2/test_mappings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626b78d",
   "metadata": {},
   "source": [
    "## training from IOB to biaffine (no offset) (i2b2 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a209268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T04:01:12.005630Z",
     "start_time": "2022-03-04T04:01:12.003244Z"
    }
   },
   "outputs": [],
   "source": [
    "# we use 2010 i2b2 data as BIO example, you can replace with any BIO formatted data here\n",
    "# example use conll-2003 (we have the data in ./test_data directory)\n",
    "# we will assume no token offset required so no mappings are needed here\n",
    "p = Path(\"./data/i2b22010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63359f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T04:01:15.252754Z",
     "start_time": "2022-03-04T04:01:15.242521Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(fn):\n",
    "    with open(fn, \"r\") as f:\n",
    "        text = f.read().strip()\n",
    "    \n",
    "    sents = text.split(\"\\n\\n\")\n",
    "    nsents = []\n",
    "    for sent in sents:\n",
    "        words = sent.strip().split(\"\\n\")\n",
    "        nsent = []\n",
    "        for i, word in enumerate(words):\n",
    "            info = word.strip().split(\" \")\n",
    "            word_text = info[0]\n",
    "            label = info[-1]\n",
    "            new_word = [word_text, label, i]\n",
    "            nsent.append(new_word)\n",
    "        nsents.append(nsent)\n",
    "    \n",
    "    return nsents\n",
    "\n",
    "\n",
    "def data2biaffine_format(data, is_test=False):\n",
    "    biaffine_data = []\n",
    "    \n",
    "    for sent in data:\n",
    "        tokens = []\n",
    "        entities = []\n",
    "        \n",
    "        label_ty = \"O\"\n",
    "        temp_text = []\n",
    "        idx_s = 1000\n",
    "        idx_e = 1000\n",
    "        \n",
    "        for word in sent:\n",
    "            tokens.append(word[0])\n",
    "            label = word[1]\n",
    "            if label == \"O\":\n",
    "                if label_ty != \"O\":\n",
    "                    entities.append([\" \".join(temp_text), label_ty, (idx_s, idx_e)])\n",
    "                label_ty = \"O\"\n",
    "                temp_text = []\n",
    "            elif label[0] == \"B\":\n",
    "                if label_ty != \"O\":\n",
    "                    entities.append([\" \".join(temp_text), label_ty, (idx_s, idx_e)])\n",
    "                    temp_text = []\n",
    "                label_ty = label[2:]\n",
    "                temp_text.append(word[0])\n",
    "                idx_s = word[-1]\n",
    "                idx_e = word[-1]\n",
    "            else:\n",
    "                # label if I-XX\n",
    "                if label_ty == label[2:]:\n",
    "                    idx_e = word[-1]\n",
    "                    temp_text.append(word[0])\n",
    "                else:\n",
    "                    entities.append([\" \".join(temp_text), label_ty, (idx_s, idx_e)])\n",
    "                    temp_text = [word[0]]\n",
    "                    label_ty = label[2:]\n",
    "                    idx_s = word[-1]\n",
    "                    idx_e = word[-1]\n",
    "        \n",
    "        biaffine_data.append(\n",
    "            {\n",
    "                \"tokens\": tokens,\n",
    "                \"entities\": entities\n",
    "            }\n",
    "        ) \n",
    "    \n",
    "    return biaffine_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3514b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:49:44.395658Z",
     "start_time": "2022-03-02T03:49:43.394422Z"
    }
   },
   "outputs": [],
   "source": [
    "train_bio = load_data(p / \"train.txt\")\n",
    "dev_bio = load_data(p / \"dev.txt\")\n",
    "test_bio = load_data(p / \"test.txt\")\n",
    "\n",
    "i2b22010_biaffine_train = data2biaffine_format(train_bio)\n",
    "i2b22010_biaffine_dev = data2biaffine_format(dev_bio)\n",
    "i2b22010_biaffine_test = data2biaffine_format(test_bio, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b421d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:49:44.400450Z",
     "start_time": "2022-03-02T03:49:44.397333Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_bio), len(i2b22010_biaffine_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d64faa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:49:44.407065Z",
     "start_time": "2022-03-02T03:49:44.401961Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 26\n",
    "print(train_bio[idx]), print(i2b22010_biaffine_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf74b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:49:44.412023Z",
     "start_time": "2022-03-02T03:49:44.408425Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 202\n",
    "dev_bio[idx], i2b22010_biaffine_dev[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c071294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T15:22:31.067585Z",
     "start_time": "2022-03-01T15:22:30.235730Z"
    }
   },
   "outputs": [],
   "source": [
    "json_dump(i2b22010_biaffine_train, \"./data/i2b22010/train.json\")\n",
    "json_dump(i2b22010_biaffine_dev, \"./data/i2b22010/dev.json\")\n",
    "json_dump(i2b22010_biaffine_test, \"./data/i2b22010/test.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
