{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing procedure for 2018 n2c2 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T15:51:50.595786Z",
     "start_time": "2021-08-06T15:51:50.593116Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import permutations, combinations\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T15:51:50.761465Z",
     "start_time": "2021-08-06T15:51:50.757078Z"
    }
   },
   "outputs": [],
   "source": [
    "def pkl_save(data, file):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pkl.dump(data, f)\n",
    "\n",
    "        \n",
    "def pkl_load(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = pkl.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_text(ifn):\n",
    "    with open(ifn, \"r\") as f:\n",
    "        txt = f.read()\n",
    "    return txt\n",
    "\n",
    "\n",
    "def save_text(text, ofn):\n",
    "    with open(ofn, \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T15:48:08.931498Z",
     "start_time": "2021-08-06T15:48:08.923272Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# https://github.com/uf-hobi-informatics-lab/NLPreprocessing (git clone this repo to local)\n",
    "sys.path.append(\"path to /NLPpreprocessing\")\n",
    "sys.path.append(\"path to /NLPreprocessing/text_process\")\n",
    "from annotation2BIO import pre_processing, read_annotation_brat, generate_BIO\n",
    "MIMICIII_PATTERN = \"\\[\\*\\*|\\*\\*\\]\"\n",
    "from sentence_tokenization import logger as l1\n",
    "from annotation2BIO import logger as l2\n",
    "l1.disabled = True\n",
    "l2.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_to_sent_mapping(nnsents, entities, idx2e):\n",
    "    loc_ens = []\n",
    "    \n",
    "    ll = len(nnsents)\n",
    "    mapping = defaultdict(list)\n",
    "    for idx, each in enumerate(entities):\n",
    "        en_label = idx2e[idx]\n",
    "        en_s = each[2][0]\n",
    "        en_e = each[2][1]\n",
    "        new_en = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < ll and nnsents[i][1][0] < en_s:\n",
    "            i += 1\n",
    "        s_s = nnsents[i][1][0]\n",
    "        s_e = nnsents[i][1][1]\n",
    "\n",
    "        if en_s == s_s:\n",
    "            mapping[en_label].append(i)\n",
    "\n",
    "            while i < ll and s_e < en_e:\n",
    "                i += 1\n",
    "                s_e = nnsents[i][1][1]\n",
    "            if s_e == en_e:\n",
    "                 mapping[en_label].append(i)\n",
    "            else:\n",
    "                mapping[en_label].append(i)\n",
    "                print(\"last index not match \", each)\n",
    "        else:\n",
    "            mapping[en_label].append(i)\n",
    "            print(\"first index not match \", each)\n",
    "\n",
    "            while i < ll and s_e < en_e:\n",
    "                i += 1\n",
    "                s_e = nnsents[i][1][1]\n",
    "            if s_e == en_e:\n",
    "                 mapping[en_label].append(i)\n",
    "            else:\n",
    "                mapping[en_label].append(i)\n",
    "                print(\"last index not match \", each)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def get_permutated_relation_pairs(eid2idx):\n",
    "    all_pairs = []\n",
    "    all_ids = [k for k, v in eid2idx.items()]\n",
    "    for e1, e2 in permutations(all_ids, 2):\n",
    "        all_pairs.append((e1, e2))\n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "procedure:\n",
    "\n",
    "1. preprocess text into sentences\n",
    "2. find each entity associated sentence idx\n",
    "3. generate entity pairs as relation candidates\n",
    "4. extract eneity associated sentences, locate entities in pair and insert special tags\n",
    "5. save generated data\n",
    "\n",
    "result output:\n",
    "\n",
    "1. keep pos predicted relations\n",
    "2. using map files to locate relation associated entities\n",
    "3. output as brat\n",
    "\"\"\"\n",
    "def validate_rels(rels, valid):\n",
    "    nrels = []\n",
    "    for rel in rels:\n",
    "        rtype = rel[0]\n",
    "        if tuple(rtype) not in valid:\n",
    "            print(\"invalid: \", rel)\n",
    "            continue\n",
    "        nrels.append(rel)\n",
    "    return nrels\n",
    "\n",
    "\n",
    "def check_tags(s1, s2):\n",
    "    assert EN1_START in s1 and EN1_END in s1, f\"tag error: {s1}\"\n",
    "    assert EN2_START in s2 and EN2_END in s2, f\"tag error: {s2}\"\n",
    "\n",
    "\n",
    "def format_relen(en, rloc, nsents):\n",
    "    if rloc == 1:\n",
    "        spec1, spec2 = EN1_START, EN1_END\n",
    "    else:\n",
    "        spec1, spec2 = EN2_START, EN2_END\n",
    "    sn1, tn1 = en[0][3]\n",
    "    sn2, tn2 = en[-1][3]\n",
    "    target_sent = nsents[sn1]\n",
    "    target_sent = [each[0] for each in target_sent]\n",
    "    ors =  \" \".join(target_sent)\n",
    "    \n",
    "    if sn1 != sn2:\n",
    "#         print(\"[!!!Warning] The entity is not in the same sentence\\n\", en)\n",
    "        tt = nsents[sn2]\n",
    "        tt = [each[0] for each in tt]\n",
    "        target_sent.insert(tn1, spec1)\n",
    "        tt.insert(tn2+1, spec2)\n",
    "        target_sent = target_sent + tt\n",
    "#         print(target_sent)\n",
    "    else:\n",
    "        target_sent.insert(tn1, spec1)\n",
    "        target_sent.insert(tn2+2, spec2)\n",
    "    \n",
    "    fs = \" \".join(target_sent)\n",
    "    \n",
    "    return sn1, sn2, fs, ors\n",
    "\n",
    "\n",
    "def gene_true_relations(rels, mappings, ens, e2i, nnsents, nsents, valid_comb, fid=None):\n",
    "    true_pairs = set()\n",
    "    pos_samples = []\n",
    "    \n",
    "    for rel in rels:\n",
    "        rel_type = rel[0]\n",
    "        enid1, enid2 = rel[1:]\n",
    "        \"\"\"\n",
    "        [['100', (15443, 15446), (16473, 16476), (231, 4), 'B-Strength'], \n",
    "        ['mg', (15447, 15449), (16477, 16479), (231, 5), 'I-Strength']] \n",
    "        [['Metoprolol', (15422, 15432), (16452, 16462), (231, 2), 'B-Drug'], \n",
    "        ['Succinate', (15433, 15442), (16463, 16472), (231, 3), 'I-Drug']]\n",
    "        \"\"\"\n",
    "        enbs1, enbe1 = mappings[enid1]\n",
    "        en1 = nnsents[enbs1: enbe1+1]\n",
    "        si1, sii1, fs1, ors1 = format_relen(en1, 1, nsents)\n",
    "        enbs2, enbe2 = mappings[enid2]\n",
    "        en2 = nnsents[enbs2: enbe2+1]\n",
    "        si2, sii2, fs2, ors2 = format_relen(en2, 2, nsents)\n",
    "        sent_diff = abs(si1 - si2)\n",
    "        \n",
    "        en1t = en1[0][-1].split(\"-\")[-1]\n",
    "        en2t = en2[0][-1].split(\"-\")[-1]\n",
    "\n",
    "        true_pairs.add((enid1, enid2))\n",
    "        \n",
    "        if (en1t, en2t) not in valid_comb:\n",
    "            continue\n",
    "        \n",
    "        if sent_diff <= CUTOFF:\n",
    "            check_tags(fs1, fs2)\n",
    "            assert (en1t, en2t) in valid_comb, f\"{en1t} {en2t}\"\n",
    "            if DO_BIN:\n",
    "                pos_samples.append((sent_diff, \"pos\", fs1, fs2, en1t, en2t, enid1, enid2, fid))\n",
    "            else:\n",
    "                pos_samples.append((sent_diff, rel_type, fs1, fs2, en1t, en2t, enid1, enid2, fid))\n",
    "\n",
    "    return pos_samples, true_pairs\n",
    "        \n",
    "\n",
    "def gene_neg_relation(perm_pairs, true_pairs, mappings, ens, e2i, nnsents, nsents, valid_comb, fid=None):\n",
    "    neg_samples = []\n",
    "    for each in perm_pairs:\n",
    "        enid1, enid2 = each\n",
    "        \n",
    "        # not in true relation\n",
    "        if (enid1, enid2) in true_pairs:\n",
    "            continue\n",
    "        \n",
    "        enc1 = ens[e2i[enid1]]\n",
    "        enc2 = ens[e2i[enid2]]\n",
    "\n",
    "        enbs1, enbe1 = mappings[enid1]\n",
    "        en1 = nnsents[enbs1: enbe1+1]\n",
    "        si1, sii1, fs1, ors1 = format_relen(en1, 1, nsents)\n",
    "        enbs2, enbe2 = mappings[enid2]\n",
    "        en2 = nnsents[enbs2: enbe2+1]\n",
    "        si2, sii2, fs2, ors2 = format_relen(en2, 2, nsents)\n",
    "        sent_diff = abs(si1 - si2)\n",
    "        \n",
    "        en1t = en1[0][-1].split(\"-\")[-1]\n",
    "        en2t = en2[0][-1].split(\"-\")[-1]\n",
    "        \n",
    "        if (en1t, en2t) not in valid_comb:\n",
    "            continue\n",
    "        \n",
    "        if sent_diff <= CUTOFF:\n",
    "            check_tags(fs1, fs2)\n",
    "            assert (en1t, en2t) in valid_comb, f\"{en1t} {en2t}\"\n",
    "            if fid:\n",
    "                neg_samples.append((sent_diff, NEG_REL, fs1, fs2, en1t, en2t, enid1, enid2, fid))\n",
    "            else:\n",
    "                neg_samples.append((sent_diff, NEG_REL, fs1, fs2, en1t, en2t, enid1, enid2))\n",
    "    \n",
    "    return neg_samples\n",
    "\n",
    "    \n",
    "def create_training_samples(file_path, valids=None, valid_comb=None):\n",
    "    fids = []\n",
    "    root = Path(file_path)\n",
    "    \n",
    "    dpos = defaultdict(list)\n",
    "    dneg = defaultdict(list)\n",
    "    \n",
    "    for txt_fn in root.glob(\"*.txt\"):\n",
    "        fids.append(txt_fn.stem)\n",
    "        ann_fn = root / (txt_fn.stem+\".ann\")\n",
    "\n",
    "        # load text\n",
    "        txt = load_text(txt_fn)\n",
    "        pre_txt, sents = pre_processing(txt_fn, deid_pattern=MIMICIII_PATTERN)\n",
    "        e2i, ens, rels = read_annotation_brat(ann_fn)\n",
    "        i2e = {v: k for k, v in e2i.items()}\n",
    "        \n",
    "        nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "        print(nsents)\n",
    "        total_len = len(nsents)\n",
    "        nnsents = [w for sent in nsents for w in sent]\n",
    "        mappings = create_entity_to_sent_mapping(nnsents, ens, i2e)\n",
    "\n",
    "        pos_samples, true_pairs = gene_true_relations(\n",
    "            rels, mappings, ens, e2i, nnsents, nsents, valid_comb, fid=txt_fn.stem)\n",
    "        perm_pairs = get_permutated_relation_pairs(e2i)\n",
    "        neg_samples = gene_neg_relation(\n",
    "            perm_pairs, true_pairs, mappings, ens, e2i, nnsents, nsents, valid_comb, fid=txt_fn.stem)\n",
    "        \n",
    "        for pos_sample in pos_samples:\n",
    "            dpos[pos_sample[0]].append(pos_sample)\n",
    "        for neg_sample in neg_samples:\n",
    "            dneg[neg_sample[0]].append(neg_sample)\n",
    "        \n",
    "    return dpos, dneg\n",
    "\n",
    "\n",
    "def create_test_samples(file_path, valids=None, valid_comb=None):\n",
    "    #create a separate mapping file\n",
    "    rel_mappings = []\n",
    "    #\n",
    "    fids = []\n",
    "    root = Path(file_path)\n",
    "    preds = defaultdict(list)\n",
    "    \n",
    "    for txt_fn in root.glob(\"*.txt\"):\n",
    "        fids.append(txt_fn.stem)\n",
    "        ann_fn = root / (txt_fn.stem + \".ann\")\n",
    "\n",
    "        # load text\n",
    "        txt = load_text(txt_fn)\n",
    "        pre_txt, sents = pre_processing(txt_fn, deid_pattern=MIMICIII_PATTERN)\n",
    "        e2i, ens, _ = read_annotation_brat(ann_fn)\n",
    "        i2e = {v: k for k, v in e2i.items()}\n",
    "        \n",
    "        nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "        total_len = len(nsents)\n",
    "        nnsents = [w for sent in nsents for w in sent]\n",
    "        mappings = create_entity_to_sent_mapping(nnsents, ens, i2e)\n",
    "        \n",
    "        perm_pairs = get_permutated_relation_pairs(e2i)\n",
    "        pred = gene_neg_relation(perm_pairs, set(), mappings, ens, e2i, nnsents, nsents, valid_comb, fid=txt_fn.stem)\n",
    "        for idx, pred_s in enumerate(pred):\n",
    "            preds[pred_s[0]].append(pred_s)\n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_sent_id(en_pos, send_bound):\n",
    "    e_s = en_pos[0]\n",
    "    e_e = en_pos[1]\n",
    "    for k, v in sent_bound.items():\n",
    "        s_s = v[0]\n",
    "        s_e = v[1]\n",
    "        if e_s >= s_s and e_s <= s_e and e_e >s_e :\n",
    "            print(\"entity is in two sentence\")\n",
    "        if e_s >= s_s and e_s <= s_e:\n",
    "            return k\n",
    "        \n",
    "\n",
    "def extract_entity_comb_for_relation(e2idx, entities, rels, sent_bound):\n",
    "    #'T1': 0\n",
    "    #'meropenem', 'Drug', (4534, 4543)\n",
    "    #('Strength-Drug', 'T5', 'T39')\n",
    "    rn = defaultdict(list)\n",
    "    rl = []\n",
    "    for rel in rels:\n",
    "        rtype = rel[0]\n",
    "        en1 = rel[1]\n",
    "        en2 = rel[2]\n",
    "        en1_type = entities[e2idx[en1]][1]\n",
    "        en2_type = entities[e2idx[en2]][1]\n",
    "        rn[rtype].append((en1_type, en2_type))\n",
    "        en1_pos = entities[e2idx[en1]][2]\n",
    "        e1_n = en_sent_id(en1_pos, sent_bound)\n",
    "        en2_pos = entities[e2idx[en2]][2]\n",
    "        e2_n = en_sent_id(en2_pos, sent_bound)\n",
    "        rl.append(abs(e1_n-e2_n))\n",
    "    return rn, rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Output strategy:\n",
    "\n",
    "1. by cross-distance\n",
    "- no cross distance; all in one\n",
    "- by cross distance; all in unique\n",
    "- by partial cross distance; within-sentence vs. cross sentence\n",
    "\n",
    "2. relation format\n",
    "- [CLS] S1 [SEP] S2 [SEP]\n",
    "- [CLS] S1 S2 [SEP]\n",
    "\n",
    "We only handle (1) in data generation here, (2) will be handled by the data_utils\n",
    "\"\"\"\n",
    "def to_tsv(data, fn):\n",
    "    header = \"\\t\".join([str(i+1) for i in range(len(data[0]))])\n",
    "    with open(fn, \"w\") as f:\n",
    "        f.write(f\"{header}\\n\")\n",
    "        for each in data:\n",
    "            d = \"\\t\".join([str(e) for e in each])\n",
    "            f.write(f\"{d}\\n\")\n",
    "\n",
    "\n",
    "def to_5_cv(data, ofd):\n",
    "    if not os.path.isdir(ofd):\n",
    "        os.mkdir(ofd)\n",
    "    \n",
    "    np.random.seed(13)\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    dfs = np.array_split(data, 5)\n",
    "    a = [0,1,2,3,4]\n",
    "    for each in combinations(a, 4):\n",
    "        b = list(set(a) - set(each))[0]\n",
    "        n = dfs[b]\n",
    "        m = []\n",
    "        for k in each:\n",
    "            m.extend(dfs[k])\n",
    "        if not os.path.isdir(os.path.join(ofd, f\"sample{b}\")):\n",
    "            os.mkdir(os.path.join(ofd, f\"sample{b}\"))\n",
    "        \n",
    "        to_tsv(m, os.path.join(ofd, f\"sample{b}\", \"train.tsv\"))\n",
    "        to_tsv(n, os.path.join(ofd, f\"sample{b}\", \"dev.tsv\"))\n",
    "\n",
    "\n",
    "def all_in_one(*dd, dn=\"2018n2c2\", do_train=True):\n",
    "    data = []\n",
    "    for d in dd:\n",
    "        for k, v in d.items():\n",
    "            for each in v:\n",
    "                data.append(each[1:])\n",
    "    \n",
    "    output_path = f\"../data/{dn}_aio_th{CUTOFF}\"\n",
    "    p = Path(output_path)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if do_train:\n",
    "        to_tsv(data, p/\"train.tsv\")\n",
    "        if OUTPUT_CV:\n",
    "            to_5_cv(data, p.as_posix())\n",
    "    else:\n",
    "        to_tsv(data, p/\"test.tsv\")\n",
    "    \n",
    "\n",
    "def all_in_unique(*dd, dn=\"2018n2c2\", do_train=True):\n",
    "    for idx in range(CUTOFF+1):\n",
    "        data = []\n",
    "        for d in dd:\n",
    "            for k, v in d.items():\n",
    "                for each in v:\n",
    "                    if k == idx:\n",
    "                        data.append(each[1:])\n",
    "        \n",
    "        output_path = f\"../data/{dn}_aiu_th{CUTOFF}\"\n",
    "        p = Path(output_path) / f\"cutoff_{idx}\"\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        if do_train:\n",
    "            to_tsv(data, p/\"train.tsv\")\n",
    "            if OUTPUT_CV:\n",
    "                to_5_cv(data, p.as_posix())\n",
    "        else:\n",
    "            to_tsv(data, p/\"test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBAL CONFIG VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We did not automated this process\n",
    "You can manually create these variables\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general pre-defined special tags\n",
    "EN1_START = \"[s1]\"\n",
    "EN1_END = \"[e1]\"\n",
    "EN2_START = \"[s2]\"\n",
    "EN2_END = \"[e2]\"\n",
    "NEG_REL = \"NonRel\"\n",
    "# max valid cross sentence distance\n",
    "CUTOFF = 1\n",
    "# output 5-fold cross validation data\n",
    "OUTPUT_CV = False\n",
    "# do binary classification (if false, then we do multiclass classification)\n",
    "DO_BIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_training = \"./data/2018_n2c2_ade/track2-training_data/\"\n",
    "n2c2_test = \"./data/2018_n2c2_ade/track2-test_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get training stats\n",
    "file_ids = set([fn.stem for fn in Path(n2c2_training).glob(\"*.ann\")])\n",
    "print(\"total training files: \", len(file_ids))\n",
    "\n",
    "relation_types = set()\n",
    "ann_rel_comb = defaultdict(list)\n",
    "rel_sent_dis = []\n",
    "for each_file in file_ids:\n",
    "    input_file = n2c2_training + \"/\" + each_file + \".txt\"\n",
    "    ann_file = n2c2_training + \"/\" +  each_file + \".ann\"\n",
    "    txt, sents = pre_processing(input_file, MIMICIII_PATTERN)\n",
    "    e2idx, entities, rels = read_annotation_brat(ann_file)\n",
    "    nsents, sent_bound = generate_BIO(sents, entities)\n",
    "    er_comb, er_n = extract_entity_comb_for_relation(e2idx, entities, rels, sent_bound)\n",
    "    rel_sent_dis.extend(er_n)\n",
    "    for k, v in er_comb.items():\n",
    "        for vv in v:\n",
    "            ann_rel_comb[k].append(vv)\n",
    "    \n",
    "    for each in rels:\n",
    "        relation_types.add(each[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comb = set()\n",
    "for k, v in ann_rel_comb.items():\n",
    "    print(k)\n",
    "    print(Counter(v))\n",
    "    all_comb.update(Counter(v).keys())\n",
    "print(\"all entity pair type combinations in training annotation:\\n\", all_comb)\n",
    "print(\"all relation types in training annotations:\\n\", relation_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid combination of entities as relations\n",
    "# only entity pairs with following type combinations will be included into training and test\n",
    "# other pairs will be excluded\n",
    "n2c2_valid_comb = {\n",
    "    ('ADE', 'Drug'), ('Reason', 'Drug'),\n",
    "    ('Strength', 'Drug'), ('Route', 'Drug'), \n",
    "    ('Frequency', 'Drug'), ('Dosage', 'Drug'),\n",
    "    ('Form', 'Drug'), ('Duration', 'Drug')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for binary classification\n",
    "# created based on valid combination\n",
    "# you need entp2rel for post-processing to generated Brat formatted output and perform evaluation\n",
    "\n",
    "entp2rel = {\n",
    "    ('ADE', 'Drug'):'ADE-Drug', \n",
    "    ('Reason', 'Drug'):'Reason-Drug',\n",
    "    ('Strength', 'Drug'):'Strength-Drug', \n",
    "    ('Route', 'Drug'):'Route-Drug', \n",
    "    ('Frequency', 'Drug'):'Frequency-Drug', \n",
    "    ('Dosage', 'Drug'):'Dosage-Drug',\n",
    "    ('Form', 'Drug'):'Form-Drug', \n",
    "    ('Duration', 'Drug'):'Duration-Drug'\n",
    "}\n",
    "\n",
    "if DO_BIN:\n",
    "    pkl_save(entp2rel, \"../data/2018n2c2_relation_processed/2018n2c2_mapping.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate .ann files with only entities\n",
    "- note:\n",
    "    - in re with annotation as input, using following code to create a dir with entity only files\n",
    "    - in end-to-end, this should be just NER results output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2c2_test_entity_only = \"./data/2018_n2c2_ade/track2-test_data_entity_only\"\n",
    "p_en_only =  Path(n2c2_test_entity_only)\n",
    "p_en_only.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for each in Path(n2c2_test).glob(\"*.ann\"):\n",
    "    output = p_en_only / each.name\n",
    "    lines = load_text(each).strip().split(\"\\n\")\n",
    "    lines = filter(lambda x: x.startswith(\"T\"), lines)\n",
    "    save_text(\"\\n\".join(lines), output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpos, dneg = create_training_samples(n2c2_training, None, n2c2_valid_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many data generated\n",
    "for k, v in dpos.items():\n",
    "    print(k, len(v))\n",
    "    \n",
    "for k, v in dneg.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "dpos[0][0], dneg[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output data\n",
    "all_in_one(dpos, dneg, dn=\"2018n2c2\", do_train=True)\n",
    "all_in_unique(dpos, dneg, dn=\"2018n2c2\", do_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = create_test_samples(n2c2_test, None, n2c2_valid_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in preds.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_one(preds, dn=\"2018n2c2\", do_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_unique(preds, dn=\"2018n2c2\", do_train=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
